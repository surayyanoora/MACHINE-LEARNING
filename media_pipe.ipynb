{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f0fab1-a70f-4464-8e5b-eb9a5fa747eb",
   "metadata": {},
   "source": [
    "MediaPipe is an open source framework developed by Google that offers customizable ML pipelines to process media data such as images,vedio and audio.it provides a wide range of pretrained ML models and components to perform various tasks including:\n",
    "\n",
    "1-pose estimation:detectng and tracking human body poses in images and vedio.\n",
    "2-Hand Tracking:identifying and tracking hands in images and vedios.\n",
    "3-Object detection:Detecting and tracking object in imges and video streams.\n",
    "4-Face detection:Detecting and trackingfaces in images and vedio\n",
    "5-face mesh:estimating facial landmarks in real time\n",
    "6-Holistic :combining multiple components like face detection,pose estimation\n",
    "7-Selfie segmentation:segmenting a persons image from background in real time\n",
    "8-Hair segmentation:segmenting hair from images and vedio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1bb451c-054c-4993-b9c4-0806dfc4f910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\yoga\\appdata\\roaming\\python\\python311\\site-packages (0.10.14)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: absl-py in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\yoga\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\yoga\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from mediapipe) (3.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\yoga\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\yoga\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\yoga\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.7)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from jax->mediapipe) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from jax->mediapipe) (1.11.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yoga\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install mediapipe --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20018f60-0bf8-4107-9eee-6e6ddfc0446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f88d246-6cf2-42af-882d-38030923aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_facedetection=mp.solutions.face_detection\n",
    "mp_drawings=mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed625f50-9df2-4bf3-979e-793a1bfb40cf",
   "metadata": {},
   "source": [
    "mp_facedetection=mp.solutions.face_detection : this line imports face detection module from the mediapipe library and assigns it to the varible mp_facedetection.This modeule provide functionality for detecting faces in images or vedios streams.\n",
    "mp_drawings=mp.solutions.drawing_utils:import drawing utilities module from mediapipe library and assign it to Variable mp_drawings .this modeule contains functions for drawing annotations such as bounding boxes or landmarks on images or vedio frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dca6c14-5f73-444a-a17a-f21ff7e48a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection=mp_facedetection.FaceDetection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b4dd3-3e12-4c75-9571-21ca2c4b1089",
   "metadata": {},
   "source": [
    "The FaceDetection classin the mediapipe library utilizes a pre-trained ML model to detect faces in images or vedio frames.this model has been trained on a large dataset of annotated images,allowing it to learn features and patterns associated with human faces,creating an instance of the face detection model from the mediapipe library.\n",
    "\n",
    "face_detection=mp_facedetection.FaceDetection():initializes an instance of the facedetection class,The instance of the facedetection class will allow to perform face detection on images or vedio streams using the functionality provided by the  mediapipe library,then use this facedetection to detect faces in images or vedios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d41f136-e8b3-4d53-9f11-7f93950f9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video=cv2.VideoCapture(0)\n",
    "while True:\n",
    "    suc,img=video.read() \n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    result=face_detection.process(img)\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n",
    "    if result.detections:\n",
    "        for det in result.detections:\n",
    "            mp_drawings.draw_detection(img,det)\n",
    "    cv2.imshow('FACE',img)\n",
    "    if cv2.waitKey(1) & 0XFF==ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee2802d1-adde-4de8-8b4f-ac1aed4d2c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[label_id: 0\n",
       " score: 0.933732\n",
       " location_data {\n",
       "   format: RELATIVE_BOUNDING_BOX\n",
       "   relative_bounding_box {\n",
       "     xmin: 0.451529324\n",
       "     ymin: 0.566522419\n",
       "     width: 0.290325046\n",
       "     height: 0.387089133\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.555330217\n",
       "     y: 0.663297653\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.677778244\n",
       "     y: 0.659185171\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.636500955\n",
       "     y: 0.739553511\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.631156385\n",
       "     y: 0.829123437\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.459326535\n",
       "     y: 0.724565744\n",
       "   }\n",
       "   relative_keypoints {\n",
       "     x: 0.719986677\n",
       "     y: 0.711164057\n",
       "   }\n",
       " }]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed5f2f8-3cad-4cef-905e-040831c58000",
   "metadata": {},
   "source": [
    "suc,img=video.read() #read video to img and is succesfully detected \"suc\"\n",
    "img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)#convert bgr to rgb color\n",
    "result=face_detection.process(img)#process function detect faces in the image store to result variable\n",
    "img=cv2.cvtColor(img,cv2.COLOR_RGB2BGR)#reconvert to bgr\n",
    "if facs are detected (if result.detections),a loop iterates through each detected face.\n",
    "mp_drawings.draw_detection(img,det):draws the detected face on the image\n",
    "cv2.imshow('FACE',img):displaying the image with detected faces.\n",
    "exit conditions cv2.waitkey(1) & 0XFF==ord('q'):breaks the loop and exit if the key 'q' is pressed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba98431-4216-4762-9809-7db87525d065",
   "metadata": {},
   "source": [
    "cv2.waitkey(1):wait for 1 millisecond for a key event.\n",
    "cv2.waitkey(1) & 0XFF : the bitwise AND operations ensures the result is written the ASCII range\n",
    "== ord('q') compares the result to the ASCII value of 'q' to check if 'q' was pressed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f297ec-1f08-494a-bd35-9ec6c5f6801a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
