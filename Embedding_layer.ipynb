{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9efc032-0892-4a70-9668-79b659819a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a7d507e-06b7-4aec-a5dd-f45b84a18bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[\n",
    "    \"This is the first document. \",\n",
    "    \"This document is the second document. \",\n",
    "    \"And this is the third one. \",\n",
    "    \"Is this the first document?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7699c1-1ae7-4bea-8b92-b55697898102",
   "metadata": {},
   "source": [
    "##### tokenize the text:split your text into individual words or tokens.\n",
    "##### create a vocabulary: build a vocabulary mapping each unique words/tokens to an integer index.\n",
    "##### covert text to a sequence:replace each word /token in the text with its corresponding integer index based on the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2eb6c75-8f84-4773-8b20-fed1e0d96a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step1:tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index= tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a255649b-4094-46d4-87e5-45ba25c6348c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 1,\n",
       " 'is': 2,\n",
       " 'the': 3,\n",
       " 'document': 4,\n",
       " 'first': 5,\n",
       " 'second': 6,\n",
       " 'and': 7,\n",
       " 'third': 8,\n",
       " 'one': 9}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d0199-c638-4850-8c38-b1c5be7f0130",
   "metadata": {},
   "source": [
    "#step2:convert text to sequences based on token number.tokenize the text using Tokenizer and convert it into sequences of integers,\n",
    "#pad the sequences to ensure they all have the same length. then create an embedding matrix using Embedding layer,\n",
    "#where each word index in the sequences is mapped to a dense vector representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3293ce6-88b3-435c-8b89-dcb58350fa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37d1786d-ff6a-4ace-961a-05dbc544599c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 5, 4], [1, 4, 2, 3, 6, 4], [7, 1, 2, 3, 8, 9], [2, 1, 3, 5, 4]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1988fa-ffe1-4585-b184-cf44028f5958",
   "metadata": {},
   "source": [
    "#then pad the sequencs to ensure they all have the same length.\n",
    "#then create an embedding matrix using embedding layer,where each word index in the sequence is mapped to a dense vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8730d874-fa87-479b-b832-cc3dde657df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step3: pad sequences(optional)\n",
    "#Ensure all sequences have the same length by padding them with zeros or truncating them.\n",
    "\n",
    "max_sequence_length = max([len(seq) for seq in sequences])\n",
    "\n",
    "sequences_padded = pad_sequences(sequences, max_sequence_length,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0af5db0e-96fd-4df9-8816-5b340d7a6e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 5, 4, 0],\n",
       "       [1, 4, 2, 3, 6, 4],\n",
       "       [7, 1, 2, 3, 8, 9],\n",
       "       [2, 1, 3, 5, 4, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da998252-b072-433d-8260-e87e9aaf32de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 4: Apply Embeddeding layer\n",
    "vocab_size = len(word_index) + 1 #Add 1 for the padding token\n",
    "embedding_dim = 10 #Dimensionality of the dense embedding\n",
    "embedding_matrix = tf.keras.layers.Embedding(vocab_size,embedding_dim)(sequences_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f432db1-c002-46a2-943d-69938e0af163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77490394-c691-41bd-82af-e47d512182e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 6, 10), dtype=float32, numpy=\n",
       "array([[[ 0.00567038,  0.02052544, -0.02147003, -0.00859245,\n",
       "         -0.02565751,  0.00093352, -0.0476921 ,  0.03640959,\n",
       "         -0.02409015,  0.04215823],\n",
       "        [-0.03412654,  0.02689386, -0.02375519,  0.00173729,\n",
       "         -0.03407858, -0.0347609 , -0.03391661, -0.02012723,\n",
       "          0.03678167, -0.017207  ],\n",
       "        [ 0.04921539,  0.04874707, -0.03589689, -0.02438277,\n",
       "         -0.01193593,  0.03276572, -0.01988528,  0.00199729,\n",
       "          0.04116702, -0.01148437],\n",
       "        [-0.04121898, -0.04724805, -0.03039035,  0.00681549,\n",
       "         -0.01322696,  0.04899949,  0.02516169, -0.04047086,\n",
       "         -0.04650396, -0.01550307],\n",
       "        [ 0.04312699, -0.02032094,  0.04697779, -0.04523547,\n",
       "          0.00775957, -0.0157824 ,  0.00147123,  0.01459128,\n",
       "          0.04694228,  0.01379362],\n",
       "        [ 0.01754178, -0.03100899,  0.00863298, -0.00800971,\n",
       "          0.03541431, -0.0290244 , -0.01782984, -0.00624729,\n",
       "          0.04921286, -0.0074969 ]],\n",
       "\n",
       "       [[ 0.00567038,  0.02052544, -0.02147003, -0.00859245,\n",
       "         -0.02565751,  0.00093352, -0.0476921 ,  0.03640959,\n",
       "         -0.02409015,  0.04215823],\n",
       "        [ 0.04312699, -0.02032094,  0.04697779, -0.04523547,\n",
       "          0.00775957, -0.0157824 ,  0.00147123,  0.01459128,\n",
       "          0.04694228,  0.01379362],\n",
       "        [-0.03412654,  0.02689386, -0.02375519,  0.00173729,\n",
       "         -0.03407858, -0.0347609 , -0.03391661, -0.02012723,\n",
       "          0.03678167, -0.017207  ],\n",
       "        [ 0.04921539,  0.04874707, -0.03589689, -0.02438277,\n",
       "         -0.01193593,  0.03276572, -0.01988528,  0.00199729,\n",
       "          0.04116702, -0.01148437],\n",
       "        [-0.03490984,  0.02583822,  0.040228  ,  0.01407709,\n",
       "          0.0336975 , -0.0205565 ,  0.03186164, -0.00416268,\n",
       "         -0.04523582,  0.00587165],\n",
       "        [ 0.04312699, -0.02032094,  0.04697779, -0.04523547,\n",
       "          0.00775957, -0.0157824 ,  0.00147123,  0.01459128,\n",
       "          0.04694228,  0.01379362]],\n",
       "\n",
       "       [[ 0.04463457, -0.00254033,  0.00069755,  0.04337962,\n",
       "         -0.04632889,  0.04540861,  0.02231793, -0.00162885,\n",
       "         -0.01672835, -0.00591321],\n",
       "        [ 0.00567038,  0.02052544, -0.02147003, -0.00859245,\n",
       "         -0.02565751,  0.00093352, -0.0476921 ,  0.03640959,\n",
       "         -0.02409015,  0.04215823],\n",
       "        [-0.03412654,  0.02689386, -0.02375519,  0.00173729,\n",
       "         -0.03407858, -0.0347609 , -0.03391661, -0.02012723,\n",
       "          0.03678167, -0.017207  ],\n",
       "        [ 0.04921539,  0.04874707, -0.03589689, -0.02438277,\n",
       "         -0.01193593,  0.03276572, -0.01988528,  0.00199729,\n",
       "          0.04116702, -0.01148437],\n",
       "        [-0.03298805,  0.04610044, -0.00370473, -0.00473063,\n",
       "         -0.03075806,  0.00194208,  0.03579778,  0.01987376,\n",
       "         -0.00871215, -0.03199943],\n",
       "        [-0.0328807 ,  0.01174214, -0.03797282,  0.02631238,\n",
       "         -0.03834025,  0.03142041, -0.0484482 , -0.01161272,\n",
       "          0.04945078,  0.04879684]],\n",
       "\n",
       "       [[-0.03412654,  0.02689386, -0.02375519,  0.00173729,\n",
       "         -0.03407858, -0.0347609 , -0.03391661, -0.02012723,\n",
       "          0.03678167, -0.017207  ],\n",
       "        [ 0.00567038,  0.02052544, -0.02147003, -0.00859245,\n",
       "         -0.02565751,  0.00093352, -0.0476921 ,  0.03640959,\n",
       "         -0.02409015,  0.04215823],\n",
       "        [ 0.04921539,  0.04874707, -0.03589689, -0.02438277,\n",
       "         -0.01193593,  0.03276572, -0.01988528,  0.00199729,\n",
       "          0.04116702, -0.01148437],\n",
       "        [-0.04121898, -0.04724805, -0.03039035,  0.00681549,\n",
       "         -0.01322696,  0.04899949,  0.02516169, -0.04047086,\n",
       "         -0.04650396, -0.01550307],\n",
       "        [ 0.04312699, -0.02032094,  0.04697779, -0.04523547,\n",
       "          0.00775957, -0.0157824 ,  0.00147123,  0.01459128,\n",
       "          0.04694228,  0.01379362],\n",
       "        [ 0.01754178, -0.03100899,  0.00863298, -0.00800971,\n",
       "          0.03541431, -0.0290244 , -0.01782984, -0.00624729,\n",
       "          0.04921286, -0.0074969 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2476959-0399-4c54-80b3-1b429fa47ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6, 10)\n"
     ]
    }
   ],
   "source": [
    "#print the embedding matrix shape\n",
    "print(embedding_matrix.shape)  #Output: (num_samples , max_sequence_length,embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de2dce1-dec0-4344-9a79-bab87ac228e8",
   "metadata": {},
   "source": [
    "Dimension 1 (4): This dimension corresponds to the number of samples in your input data. In this case, you have 4 samples (or sentences/documents). Dimension 2 (6): This dimension represents the length of the sequences after padding. Since you've padded the sequences to a length of 6, this dimension is Dimension 3 (100): This dimension is the dimensionality of the dense embedding vectors. You've chosen an embedding dimension of 100, so each word is represented by a dense vector of length 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18509062-6120-432e-aaf4-b687c10a9e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
